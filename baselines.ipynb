{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c14c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Importing our own\n",
    "from models.trainer import TrainModelWrapper\n",
    "from models.classification import Classification, Classification_Dropout\n",
    "from models.regression import Regression, Regression_Dropout\n",
    "import utils.datasets as DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab34e42",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab61e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DB.Datasets()\n",
    "train_mnist, test_mnist = datasets.get_MNIST()\n",
    "train_mnist, val_mnist = torch.utils.data.random_split(train_mnist, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea63219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Class Problem\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_nodes, num_classes, dropout=False):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_nodes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_nodes,hidden_nodes)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(hidden_nodes,num_classes)\n",
    "        if dropout:\n",
    "            self.drop1 = nn.Dropout(0.2)\n",
    "            self.drop2 = nn.Dropout(0.5)\n",
    "            self.drop3 = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        if self.dropout:\n",
    "            x = self.drop1(x)\n",
    "        out = self.linear1(x)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.drop2(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.drop3(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.linear3(out)\n",
    "        # NO SOFTMAX since done by crossentropyloss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a6439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_classification(nodes_hidden, learning_rate, name, dropout=False):\n",
    "    num_epochs = 600\n",
    "    batch_size = 128\n",
    "    \n",
    "    if not dropout:\n",
    "        model = Classification(input_dim=784, output_dim=10, hl_units=nodes_hidden, hl_type=nn.Linear)\n",
    "    else:\n",
    "        model = Classification_Dropout(input_dim=784, output_dim=10, hl_units=nodes_hidden, hl_type=nn.Linear)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    args = {\n",
    "            'model': model,\n",
    "            'model_name':name,\n",
    "            'train_dataset': train_mnist,\n",
    "            'criterion': criterion,\n",
    "            'batch_size': batch_size,\n",
    "            'optimizer': optimizer,\n",
    "            'es_flag': False,\n",
    "            'num_epochs': num_epochs,\n",
    "            'val_dataset': val_mnist,\n",
    "            'test_dataset': test_mnist,\n",
    "            'mode': 2\n",
    "            }\n",
    "    trainer = TrainModelWrapper(**args)\n",
    "    model, history = trainer.train()\n",
    "    \n",
    "    torch.save(model.state_dict(), 'weights/'+name+'.pth')\n",
    "    return model, history, trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ba177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:40:42,639 [WARNING] Scheduler Object not found amongst the Arguments. Ignore warning if scheduler wasn't meant to be in the loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400_1e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:40:44,464 [INFO] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-02-24 20:40:44,465 [INFO] NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard model name corresponding to this model is 400_1e-03_20230224204042639093\n",
      "Epoch 1/600\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 391/391 [00:06<00:00, 58.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5755\n",
      "train Acc: 0.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 76.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5755\n",
      "val Acc: 0.0890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 75.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.5755\n",
      "test Acc: 0.0869\n",
      "\n",
      "Epoch 2/600\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 68.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5755\n",
      "train Acc: 0.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 72.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5755\n",
      "val Acc: 0.0890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 75.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.5755\n",
      "test Acc: 0.0869\n",
      "\n",
      "Epoch 3/600\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 68.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5755\n",
      "train Acc: 0.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 73.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5755\n",
      "val Acc: 0.0890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3, 1e-4, 1e-5]\n",
    "layers = [400, 800, 1200]\n",
    "\n",
    "for lr in lrs:\n",
    "    for hidden_size in layers:\n",
    "        _name = f\"{hidden_size}_{lr:.0e}\"\n",
    "        print(_name)\n",
    "        train_save_classification(nodes_hidden=hidden_size, learning_rate=lr, name=_name)\n",
    "        \n",
    "for lr in lrs:\n",
    "    for hidden_size in layers:\n",
    "        _name = f\"drop_{hidden_size}_{lr:.0e}\"\n",
    "        print(_name)\n",
    "        train_save_classification(nodes_hidden=hidden_size, learning_rate=lr, name=_name, dropout=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9de8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model again and continuing training is possible\n",
    "\"\"\"\n",
    "new_model = MLPModel(input_dim=784, output_dim=10, hidden_layer=400)\n",
    "new_model.load_state_dict(torch.load('weights/400_1e-03.pth'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(new_model.parameters(),lr=1e-3)\n",
    "\n",
    "args = {\n",
    "        'model': new_model,\n",
    "        'train_dataset': train_mnist,\n",
    "        'criterion': criterion,\n",
    "        'batch_size': 128,\n",
    "        'optimizer': optimizer,\n",
    "        'es_flag': False,\n",
    "        'num_epochs': 10,\n",
    "        'val_dataset': val_mnist,\n",
    "        'mode': 2\n",
    "        }\n",
    "trainer = TrainModelWrapper(**args)\n",
    "model, history = trainer.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35626853",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859998d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DB.Datasets()\n",
    "train_regr, test_regr = datasets.get_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(x=np.array(train_regr)[:,0], y=np.array(train_regr)[:,1], color='r')\n",
    "#plt.scatter(x=np.array(test_regr)[:,0], y=np.array(test_regr)[:,1], color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08537d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_regression(nodes_hidden, learning_rate, name, dropout=False):\n",
    "    num_epochs = 600\n",
    "    batch_size = 128\n",
    "    \n",
    "    if not dropout:\n",
    "        model = Regression(input_dim=1, output_dim=1, hl_units=nodes_hidden, hl_type=nn.Linear)\n",
    "    else:\n",
    "        model = Regression_Dropout(input_dim=1, output_dim=1, hl_units=nodes_hidden, hl_type=nn.Linear)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    args = {\n",
    "            'model': model,\n",
    "            'model_name': name,\n",
    "            'train_dataset': train_regr,\n",
    "            'criterion': criterion,\n",
    "            'batch_size': batch_size,\n",
    "            'optimizer': optimizer,\n",
    "            'es_flag': False,\n",
    "            'num_epochs': num_epochs,\n",
    "            'val_dataset': test_regr,\n",
    "            'test_dataset': test_regr,\n",
    "            'mode': 0\n",
    "            }\n",
    "    trainer = TrainModelWrapper(**args)\n",
    "    model, history = trainer.train()\n",
    "    \n",
    "    torch.save(model.state_dict(), 'weights/'+name+'.pth')\n",
    "    return model, history, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5928cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-3, 1e-4, 1e-5]\n",
    "layers = [400, 800, 1200]\n",
    "\n",
    "for lr in lrs:\n",
    "    for hidden_size in layers:\n",
    "        _name = f\"regr_{hidden_size}_{lr:.0e}\"\n",
    "        print(_name)\n",
    "        train_save_regression(nodes_hidden=hidden_size, learning_rate=lr, name=_name)\n",
    "        \n",
    "for lr in lrs:\n",
    "    for hidden_size in layers:\n",
    "        _name = f\"regr_drop_{hidden_size}_{lr:.0e}\"\n",
    "        print(_name)\n",
    "        train_save_regression(nodes_hidden=hidden_size, learning_rate=lr, name=_name, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "m, h, t = train_save_classification(\n",
    "    nodes_hidden=800, learning_rate=1e-3, name=\"test_class\", dropout=True\n",
    ")\n",
    "predc = t.predict(test_mnist, out_dim=10)\n",
    "\n",
    "\n",
    "m, h, t = train_save_regression(nodes_hidden=400, learning_rate=1e-3, name=\"test\")\n",
    "predr = t.predict(test_regr, out_dim=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65202b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
